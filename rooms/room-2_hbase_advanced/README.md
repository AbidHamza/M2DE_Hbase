# Room 2 : HBase avancÃ©

## Objectifs de cette room

Ã€ la fin de cette room, vous saurez :
- Comprendre et utiliser les versions et l'historique des donnÃ©es HBase
- MaÃ®triser les filtres avancÃ©s pour interroger efficacement vos donnÃ©es
- Apprendre Ã  optimiser les performances avec un bon design de row keys
- Travailler avec des donnÃ©es temporelles (logs IoT) de maniÃ¨re efficace


---

## Rappels thÃ©oriques - EXPLICATION APPROFONDIE

### Versions et historique - Comprendre en profondeur

**HBase stocke plusieurs versions de chaque cellule !**

#### Qu'est-ce qu'une version ?

Imaginez que vous modifiez la tempÃ©rature d'un capteur plusieurs fois :
- **Version 1** : 22.5Â°C Ã  10:00
- **Version 2** : 22.8Â°C Ã  10:05
- **Version 3** : 23.1Â°C Ã  10:10
- **Version 4** : 22.9Â°C Ã  10:15

HBase garde **toutes ces versions** avec leurs timestamps ! Par dÃ©faut, il garde **3 versions**, mais vous pouvez changer cela.

#### Pourquoi c'est utile ?

1. **Audit** : Voir qui a modifiÃ© quoi et quand
2. **RÃ©cupÃ©ration** : RÃ©cupÃ©rer une valeur Ã  un moment prÃ©cis dans le passÃ©
3. **Analyse** : Analyser l'Ã©volution des donnÃ©es dans le temps
4. **SÃ©curitÃ©** : DÃ©tecter des modifications suspectes

#### Comment Ã§a fonctionne ?

Chaque `put` crÃ©e une **nouvelle version** avec un **timestamp** automatique. Les versions sont triÃ©es par timestamp dÃ©croissant (la plus rÃ©cente en premier).

**Exemple concret :**
```hbase
put 'sensor_data', 'SENSOR001', 'readings:temperature', '22.5'
# Attendre quelques secondes...
put 'sensor_data', 'SENSOR001', 'readings:temperature', '22.8'
# Attendre quelques secondes...
put 'sensor_data', 'SENSOR001', 'readings:temperature', '23.1'
```

Maintenant, la cellule `readings:temperature` a **3 versions** :
- Version 1 (la plus rÃ©cente) : 23.1Â°C
- Version 2 : 22.8Â°C
- Version 3 (la plus ancienne) : 22.5Â°C

**Par dÃ©faut, `get` retourne seulement la version la plus rÃ©cente !**

### Filtres avancÃ©s - Guide complet

HBase propose de nombreux filtres pour interroger efficacement vos donnÃ©es. Voici les principaux :

#### 1. RowFilter - Filtrer sur les row keys

**UtilitÃ©** : Trouver toutes les lignes dont la row key correspond Ã  un pattern.

**Exemple** : Trouver tous les logs du device DEV001
```hbase
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.RowFilter
import org.apache.hadoop.hbase.filter.RegexStringComparator
import org.apache.hadoop.hbase.util.Bytes

scan 'iot_logs', {
  FILTER => RowFilter.new(
    CompareFilter::CompareOp.valueOf('EQUAL'),
    RegexStringComparator.new('DEV001_.*')
  )
}
```

**Explication** :
- `RowFilter` = filtre sur les row keys
- `RegexStringComparator` = utilise une expression rÃ©guliÃ¨re
- `'DEV001_.*'` = commence par "DEV001_" suivi de n'importe quoi

#### 2. ValueFilter - Filtrer sur les valeurs

**UtilitÃ©** : Trouver toutes les lignes oÃ¹ une valeur correspond Ã  un critÃ¨re.

**Exemple** : Trouver tous les logs oÃ¹ la tempÃ©rature dÃ©passe 23Â°C
```hbase
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.ValueFilter
import org.apache.hadoop.hbase.filter.BinaryComparator
import org.apache.hadoop.hbase.util.Bytes

scan 'iot_logs', {
  FILTER => ValueFilter.new(
    CompareFilter::CompareOp.valueOf('GREATER'),
    BinaryComparator.new(Bytes.toBytes('23'))
  )
}
```

**ATTENTION** : `ValueFilter` scanne **toutes les colonnes** de toutes les lignes. C'est trÃ¨s lent sur de grandes tables !

#### 3. SingleColumnValueFilter - Filtrer sur une colonne spÃ©cifique

**UtilitÃ©** : Filtrer sur la valeur d'une colonne prÃ©cise (beaucoup plus efficace que ValueFilter).

**Exemple** : Trouver tous les logs oÃ¹ `data:temperature` > 23
```hbase
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.SingleColumnValueFilter
import org.apache.hadoop.hbase.filter.BinaryComparator
import org.apache.hadoop.hbase.util.Bytes

scan 'iot_logs', {
  FILTER => SingleColumnValueFilter.new(
    Bytes.toBytes('data'),
    Bytes.toBytes('temperature'),
    CompareFilter::CompareOp.valueOf('GREATER'),
    BinaryComparator.new(Bytes.toBytes('23'))
  )
}
```

**Avantage** : HBase lit seulement la colonne spÃ©cifiÃ©e, pas toutes les colonnes !

#### 4. ColumnPrefixFilter - Filtrer sur le prÃ©fixe des colonnes

**UtilitÃ©** : RÃ©cupÃ©rer seulement les colonnes qui commencent par un prÃ©fixe.

**Exemple** : RÃ©cupÃ©rer toutes les colonnes commenÃ§ant par "temp"
```hbase
import org.apache.hadoop.hbase.filter.ColumnPrefixFilter
import org.apache.hadoop.hbase.util.Bytes

scan 'iot_logs', {
  FILTER => ColumnPrefixFilter.new(Bytes.toBytes('temp'))
}
```

#### 5. PageFilter - Limiter le nombre de rÃ©sultats

**UtilitÃ©** : Limiter le nombre de lignes retournÃ©es (pagination).

**Exemple** : RÃ©cupÃ©rer seulement les 10 premiÃ¨res lignes
```hbase
import org.apache.hadoop.hbase.filter.PageFilter

scan 'iot_logs', {
  FILTER => PageFilter.new(10)
}
```

### Optimisation - Design des row keys

**Le design des row keys est CRUCIAL pour les performances !**

#### Mauvais design : Row keys sÃ©quentielles

```hbase
# MAUVAIS
put 'table', '1', 'cf:col', 'value'
put 'table', '2', 'cf:col', 'value'
put 'table', '3', 'cf:col', 'value'
```

**ProblÃ¨me** : Toutes les insertions vont au mÃªme RegionServer â†’ **hotspot** !

#### Bon design : Row keys avec prÃ©fixes

```hbase
# BON
put 'table', 'REGION1_001', 'cf:col', 'value'
put 'table', 'REGION2_001', 'cf:col', 'value'
put 'table', 'REGION3_001', 'cf:col', 'value'
```

**Avantage** : Les donnÃ©es sont distribuÃ©es sur plusieurs RegionServers.

#### Excellent design : Row keys temporelles inversÃ©es

Pour les donnÃ©es temporelles, utilisez des timestamps inversÃ©s :

```hbase
# EXCELLENT pour donnÃ©es temporelles
put 'table', 'DEV001_20240101100000', 'cf:col', 'value'  # Format normal
put 'table', 'DEV001_99999999999999_20240101100000', 'cf:col', 'value'  # InversÃ©
```

**Avantage** : Les donnÃ©es les plus rÃ©centes sont accessibles rapidement.

**Format suggÃ©rÃ© pour donnÃ©es IoT** :
- `DEVICEID_TIMESTAMP` : `DEV001_20240101100000`
- Permet de filtrer par device ET par pÃ©riode avec `STARTROW` et `STOPROW`

---

## Exercices pratiques - GUIDE PAS Ã€ PAS DÃ‰TAILLÃ‰

### Exercice 1 : Travailler avec les versions - GUIDE COMPLET

**Objectif** : Comprendre comment HBase gÃ¨re les versions et l'historique.

#### Ã‰tape 1.1 : CrÃ©er une table avec plusieurs versions

**Entrez dans le shell HBase :**
```bash
docker exec -it $(docker compose ps -q hbase) hbase shell
```

**CrÃ©ez une table avec 5 versions conservÃ©es :**
```hbase
create 'sensor_data', {NAME => 'readings', VERSIONS => 5}
```

**Explication DÃ‰TAILLÃ‰E :**
- `create` = crÃ©er une table
- `'sensor_data'` = nom de la table
- `{NAME => 'readings', VERSIONS => 5}` = syntaxe spÃ©ciale HBase
  - `NAME => 'readings'` = nom de la famille de colonnes
  - `VERSIONS => 5` = garder 5 versions au lieu de 3 (par dÃ©faut)

**RÃ©sultat attendu :**
```
0 row(s) in 1.2345 seconds

=> Hbase::Table - sensor_data
```

#### Ã‰tape 1.2 : InsÃ©rer plusieurs valeurs avec des timestamps diffÃ©rents

**InsÃ©rez la premiÃ¨re valeur :**
```hbase
put 'sensor_data', 'SENSOR001_20240101_100000', 'readings:temperature', '22.5'
```

**Attendez 2-3 secondes, puis insÃ©rez une nouvelle valeur :**
```hbase
put 'sensor_data', 'SENSOR001_20240101_100000', 'readings:temperature', '22.8'
```

**Attendez encore 2-3 secondes, puis insÃ©rez une troisiÃ¨me valeur :**
```hbase
put 'sensor_data', 'SENSOR001_20240101_100000', 'readings:temperature', '23.1'
```

**Et une quatriÃ¨me :**
```hbase
put 'sensor_data', 'SENSOR001_20240101_100000', 'readings:temperature', '22.9'
```

**Explication :**
- MÃªme row key (`SENSOR001_20240101_100000`)
- MÃªme colonne (`readings:temperature`)
- Valeurs diffÃ©rentes Ã  des moments diffÃ©rents
- Chaque `put` crÃ©e une nouvelle version avec un nouveau timestamp

#### Ã‰tape 1.3 : Voir seulement la version la plus rÃ©cente (comportement par dÃ©faut)

**Commande :**
```hbase
get 'sensor_data', 'SENSOR001_20240101_100000', {COLUMN => 'readings:temperature'}
```

**RÃ©sultat attendu :**
```
COLUMN                   CELL
 readings:temperature    timestamp=..., value=22.9
1 row(s) in 0.1234 seconds
```

**Observation :** Seule la version la plus rÃ©cente (22.9) est affichÃ©e !

#### Ã‰tape 1.4 : Voir toutes les versions

**Commande :**
```hbase
get 'sensor_data', 'SENSOR001_20240101_100000', {COLUMN => 'readings:temperature', VERSIONS => 5}
```

**Explication DÃ‰TAILLÃ‰E :**
- `get` = rÃ©cupÃ©rer
- `'sensor_data'` = table
- `'SENSOR001_20240101_100000'` = row key
- `{COLUMN => 'readings:temperature', VERSIONS => 5}` = options
  - `COLUMN => 'readings:temperature'` = colonne spÃ©cifique
  - `VERSIONS => 5` = afficher jusqu'Ã  5 versions

**RÃ©sultat attendu :**
```
COLUMN                   CELL
 readings:temperature    timestamp=1704110400000, value=22.9
 readings:temperature    timestamp=1704110370000, value=23.1
 readings:temperature    timestamp=1704110340000, value=22.8
 readings:temperature    timestamp=1704110310000, value=22.5
4 row(s) in 0.1234 seconds
```

**Observation :** Vous voyez maintenant toutes les versions, triÃ©es par timestamp dÃ©croissant (plus rÃ©cent en premier) !

#### Ã‰tape 1.5 : RÃ©cupÃ©rer une valeur Ã  un timestamp spÃ©cifique

**Notez un timestamp de la sortie prÃ©cÃ©dente (par exemple : `1704110340000`), puis :**

```hbase
get 'sensor_data', 'SENSOR001_20240101_100000', {
  COLUMN => 'readings:temperature',
  TIMESTAMP => 1704110340000
}
```

**RÃ©sultat attendu :** Seulement la valeur correspondant Ã  ce timestamp exact (22.8 dans cet exemple).

**ðŸŽ‰ Exercice 1 terminÃ© !** Vous comprenez maintenant comment fonctionnent les versions dans HBase.

---

### Exercice 2 : Charger des donnÃ©es IoT - GUIDE COMPLET

**Objectif** : Charger des donnÃ©es rÃ©elles depuis un fichier CSV dans HBase.

#### Ã‰tape 2.1 : Examiner le fichier source

**AccÃ©dez au conteneur Hadoop :**
```bash
docker exec -it $(docker compose ps -q hadoop) bash
```

**VÃ©rifiez que le fichier existe :**
```bash
cat /data/resources/iot-logs/sample-logs.csv | head -5
```

**RÃ©sultat attendu :**
```
timestamp,device_id,temperature,humidity,location,status
2024-01-01 10:00:00,DEV001,22.5,65.2,Paris,active
2024-01-01 10:05:00,DEV002,23.1,63.8,Lyon,active
2024-01-01 10:10:00,DEV001,22.8,65.5,Paris,active
2024-01-01 10:15:00,DEV003,21.9,67.1,Marseille,active
```

**Structure du fichier :**
- Colonne 1 : `timestamp` (format : YYYY-MM-DD HH:MM:SS)
- Colonne 2 : `device_id` (ex: DEV001)
- Colonne 3 : `temperature` (dÃ©cimal)
- Colonne 4 : `humidity` (dÃ©cimal)
- Colonne 5 : `location` (ville)
- Colonne 6 : `status` (active/inactive)

#### Ã‰tape 2.2 : CrÃ©er la table HBase

**Retournez dans le shell HBase :**
```bash
exit  # Sortir du conteneur Hadoop
docker exec -it $(docker compose ps -q hbase) hbase shell
```

**CrÃ©ez la table `iot_logs` :**
```hbase
create 'iot_logs', 'data'
```

**Explication :**
- Une seule famille `data` pour toutes les colonnes
- Row key sera construite manuellement : `DEVICEID_TIMESTAMP`

#### Ã‰tape 2.3 : Charger les donnÃ©es manuellement (mÃ©thode simple)

**Pour chaque ligne du CSV, crÃ©ez une row key et insÃ©rez les donnÃ©es :**

**Ligne 1 :**
```hbase
put 'iot_logs', 'DEV001_20240101100000', 'data:temperature', '22.5'
put 'iot_logs', 'DEV001_20240101100000', 'data:humidity', '65.2'
put 'iot_logs', 'DEV001_20240101100000', 'data:location', 'Paris'
put 'iot_logs', 'DEV001_20240101100000', 'data:status', 'active'
```

**Ligne 2 :**
```hbase
put 'iot_logs', 'DEV002_20240101100500', 'data:temperature', '23.1'
put 'iot_logs', 'DEV002_20240101100500', 'data:humidity', '63.8'
put 'iot_logs', 'DEV002_20240101100500', 'data:location', 'Lyon'
put 'iot_logs', 'DEV002_20240101100500', 'data:status', 'active'
```

**Continuez pour au moins 10 lignes...**

**Format de row key :**
- `DEVICEID_YYYYMMDDHHMMSS`
- Exemple : `DEV001_20240101100000` = Device DEV001 le 2024-01-01 Ã  10:00:00

**Conversion du timestamp :**
- `2024-01-01 10:00:00` â†’ `20240101100000`
- `2024-01-01 10:05:00` â†’ `20240101100500`
- `2024-01-01 10:10:00` â†’ `20240101101000`

#### Ã‰tape 2.4 : VÃ©rifier les donnÃ©es chargÃ©es

**Comptez les lignes :**
```hbase
count 'iot_logs'
```

**RÃ©sultat attendu :** Au moins 10 lignes.

**Voir quelques exemples :**
```hbase
scan 'iot_logs', {LIMIT => 5}
```

**ðŸŽ‰ Exercice 2 terminÃ© !** Vous avez chargÃ© des donnÃ©es IoT dans HBase.

---

### Exercice 3 : Filtres avancÃ©s - GUIDE COMPLET

**Objectif** : MaÃ®triser les diffÃ©rents types de filtres HBase.

#### Ã‰tape 3.1 : RowFilter - Filtrer par device

**Trouvez tous les logs du device DEV001 :**

```hbase
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.RowFilter
import org.apache.hadoop.hbase.filter.RegexStringComparator
import org.apache.hadoop.hbase.util.Bytes

scan 'iot_logs', {
  FILTER => RowFilter.new(
    CompareFilter::CompareOp.valueOf('EQUAL'),
    RegexStringComparator.new('DEV001_.*')
  )
}
```

**Explication DÃ‰TAILLÃ‰E :**
- `import` = importer les classes nÃ©cessaires (une seule fois par session)
- `RowFilter` = filtre sur les row keys
- `RegexStringComparator` = utilise une expression rÃ©guliÃ¨re
- `'DEV001_.*'` = pattern qui signifie "commence par DEV001_ suivi de n'importe quoi"
- `EQUAL` = correspondance exacte avec le pattern

**RÃ©sultat attendu :** Tous les logs du device DEV001.

#### Ã‰tape 3.2 : ValueFilter - Filtrer par valeur de tempÃ©rature

**âš ï¸ ATTENTION : Ce filtre est LENT car il scanne toutes les colonnes !**

```hbase
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.ValueFilter
import org.apache.hadoop.hbase.filter.BinaryComparator
import org.apache.hadoop.hbase.util.Bytes

scan 'iot_logs', {
  FILTER => ValueFilter.new(
    CompareFilter::CompareOp.valueOf('GREATER'),
    BinaryComparator.new(Bytes.toBytes('23'))
  )
}
```

**Explication :**
- `ValueFilter` = filtre sur les valeurs (toutes colonnes confondues)
- `GREATER` = supÃ©rieur Ã 
- `BinaryComparator.new(Bytes.toBytes('23'))` = comparer avec la valeur "23"

**RÃ©sultat attendu :** Toutes les lignes oÃ¹ au moins une colonne a une valeur > "23".

#### Ã‰tape 3.3 : SingleColumnValueFilter - Filtrer sur une colonne spÃ©cifique

**âœ… RECOMMANDÃ‰ : Beaucoup plus efficace que ValueFilter !**

```hbase
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.SingleColumnValueFilter
import org.apache.hadoop.hbase.filter.BinaryComparator
import org.apache.hadoop.hbase.util.Bytes

scan 'iot_logs', {
  FILTER => SingleColumnValueFilter.new(
    Bytes.toBytes('data'),
    Bytes.toBytes('temperature'),
    CompareFilter::CompareOp.valueOf('GREATER'),
    BinaryComparator.new(Bytes.toBytes('23'))
  )
}
```

**Explication DÃ‰TAILLÃ‰E :**
- `SingleColumnValueFilter` = filtre sur une colonne spÃ©cifique
- `Bytes.toBytes('data')` = famille de colonnes
- `Bytes.toBytes('temperature')` = nom de la colonne
- `GREATER` = supÃ©rieur Ã 
- `BinaryComparator.new(Bytes.toBytes('23'))` = valeur de comparaison

**Avantage :** HBase lit seulement la colonne `data:temperature`, pas toutes les colonnes !

**RÃ©sultat attendu :** Toutes les lignes oÃ¹ `data:temperature` > 23.

#### Ã‰tape 3.4 : ColumnPrefixFilter - Filtrer par prÃ©fixe de colonne

**RÃ©cupÃ©rez seulement les colonnes commenÃ§ant par "temp" :**

```hbase
import org.apache.hadoop.hbase.filter.ColumnPrefixFilter
import org.apache.hadoop.hbase.util.Bytes

scan 'iot_logs', {
  FILTER => ColumnPrefixFilter.new(Bytes.toBytes('temp'))
}
```

**RÃ©sultat attendu :** Seulement les colonnes `data:temperature` (si elles existent).

#### Ã‰tape 3.5 : Combiner plusieurs filtres

**Trouvez les logs de DEV001 oÃ¹ la tempÃ©rature > 23 :**

```hbase
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.RowFilter
import org.apache.hadoop.hbase.filter.SingleColumnValueFilter
import org.apache.hadoop.hbase.filter.FilterList
import org.apache.hadoop.hbase.filter.RegexStringComparator
import org.apache.hadoop.hbase.filter.BinaryComparator
import org.apache.hadoop.hbase.util.Bytes

scan 'iot_logs', {
  FILTER => FilterList.new([
    RowFilter.new(
      CompareFilter::CompareOp.valueOf('EQUAL'),
      RegexStringComparator.new('DEV001_.*')
    ),
    SingleColumnValueFilter.new(
      Bytes.toBytes('data'),
      Bytes.toBytes('temperature'),
      CompareFilter::CompareOp.valueOf('GREATER'),
      BinaryComparator.new(Bytes.toBytes('23'))
    )
  ])
}
```

**Explication :**
- `FilterList` = combine plusieurs filtres
- Les filtres sont appliquÃ©s avec un ET logique (les deux conditions doivent Ãªtre vraies)

**ðŸŽ‰ Exercice 3 terminÃ© !** Vous maÃ®trisez maintenant les filtres avancÃ©s.

---

### Exercice 4 : RequÃªtes temporelles - GUIDE COMPLET

**Objectif** : Utiliser STARTROW et STOPROW pour des requÃªtes temporelles efficaces.

#### Ã‰tape 4.1 : Comprendre STARTROW et STOPROW

**STARTROW et STOPROW permettent de scanner seulement une plage de row keys !**

**Avantage :** Beaucoup plus rapide qu'un scan complet car HBase sait exactement oÃ¹ commencer et oÃ¹ s'arrÃªter.

#### Ã‰tape 4.2 : RÃ©cupÃ©rer les logs d'une pÃ©riode spÃ©cifique

**RÃ©cupÃ©rez tous les logs entre 10:00 et 11:00 pour DEV001 :**

```hbase
scan 'iot_logs', {
  STARTROW => 'DEV001_20240101100000',
  STOPROW => 'DEV001_20240101110000'
}
```

**Explication DÃ‰TAILLÃ‰E :**
- `STARTROW` = row key de dÃ©but (incluse)
- `STOPROW` = row key de fin (exclue)
- HBase scanne toutes les lignes entre ces deux row keys (ordre alphabÃ©tique)

**Format des row keys :**
- `DEV001_20240101100000` = DEV001 Ã  10:00:00
- `DEV001_20240101110000` = DEV001 Ã  11:00:00

**RÃ©sultat attendu :** Tous les logs de DEV001 entre 10:00 et 11:00.

#### Ã‰tape 4.3 : RÃ©cupÃ©rer les logs de plusieurs devices sur une pÃ©riode

**RÃ©cupÃ©rez tous les logs de tous les devices entre 10:00 et 11:00 :**

```hbase
scan 'iot_logs', {
  STARTROW => 'DEV001_20240101100000',
  STOPROW => 'DEV999_20240101110000'
}
```

**Explication :**
- `DEV001_...` = premier device possible
- `DEV999_...` = dernier device possible (avant DEV1000)
- Cela capture tous les devices DEV001 Ã  DEV999

#### Ã‰tape 4.4 : Combiner avec des filtres

**RÃ©cupÃ©rez les logs de DEV001 entre 10:00 et 11:00 oÃ¹ la tempÃ©rature > 23 :**

```hbase
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.SingleColumnValueFilter
import org.apache.hadoop.hbase.filter.BinaryComparator
import org.apache.hadoop.hbase.util.Bytes

scan 'iot_logs', {
  STARTROW => 'DEV001_20240101100000',
  STOPROW => 'DEV001_20240101110000',
  FILTER => SingleColumnValueFilter.new(
    Bytes.toBytes('data'),
    Bytes.toBytes('temperature'),
    CompareFilter::CompareOp.valueOf('GREATER'),
    BinaryComparator.new(Bytes.toBytes('23'))
  )
}
```

**ðŸŽ‰ Exercice 4 terminÃ© !** Vous savez maintenant faire des requÃªtes temporelles efficaces.

---

### Exercice 5 : Optimisation et analyse - GUIDE COMPLET

**Objectif** : Analyser vos donnÃ©es et optimiser les performances.

#### Ã‰tape 5.1 : Analyser la distribution des donnÃ©es

**Comptez le nombre de lignes par device :**

```hbase
# Device DEV001
scan 'iot_logs', {
  STARTROW => 'DEV001_',
  STOPROW => 'DEV002_'
}
# Notez le nombre de lignes

# Device DEV002
scan 'iot_logs', {
  STARTROW => 'DEV002_',
  STOPROW => 'DEV003_'
}
# Notez le nombre de lignes
```

**Analysez :**
- Les donnÃ©es sont-elles Ã©quitablement distribuÃ©es ?
- Y a-t-il des devices avec beaucoup plus de donnÃ©es que d'autres ?

#### Ã‰tape 5.2 : Identifier les hotspots

**Un hotspot = concentration de donnÃ©es sur un seul RegionServer**

**Signes de hotspot :**
- Beaucoup de row keys sÃ©quentielles (ex: 1, 2, 3...)
- Toutes les insertions vont au mÃªme endroit
- Un RegionServer est surchargÃ©

**VÃ©rifiez vos row keys :**
```hbase
scan 'iot_logs', {LIMIT => 20}
```

**Analysez :**
- Les row keys sont-elles bien distribuÃ©es ?
- Y a-t-il des patterns qui crÃ©ent des hotspots ?

#### Ã‰tape 5.3 : Proposer des optimisations

**Si vous avez identifiÃ© des problÃ¨mes, proposez des amÃ©liorations :**

**Exemple 1 : Ajouter un prÃ©fixe de hash**
```
Avant : DEV001_20240101100000
AprÃ¨s : 0_DEV001_20240101100000  (hash de DEV001 = 0)
```

**Exemple 2 : Inverser le timestamp**
```
Avant : DEV001_20240101100000
AprÃ¨s : DEV001_99999999999999_20240101100000  (timestamp inversÃ©)
```

#### Ã‰tape 5.4 : Tester les performances

**Comparez les temps d'exÃ©cution :**

```hbase
# Test 1 : Scan complet
# Notez le temps
scan 'iot_logs'

# Test 2 : Scan avec STARTROW/STOPROW
# Notez le temps
scan 'iot_logs', {
  STARTROW => 'DEV001_20240101100000',
  STOPROW => 'DEV001_20240101110000'
}

# Test 3 : Scan avec filtre
# Notez le temps
scan 'iot_logs', {
  FILTER => SingleColumnValueFilter.new(...)
}
```

**Analysez :**
- Quelle mÃ©thode est la plus rapide ?
- Les filtres ralentissent-ils beaucoup ?

**ðŸŽ‰ Exercice 5 terminÃ© !** Vous comprenez maintenant l'optimisation HBase.

---

## ðŸ“ Fichiers Ã  complÃ©ter

CrÃ©ez les fichiers suivants dans ce dossier (`rooms/room-2_hbase_advanced/`) :

### 1. `room-2_exercices.md`

Documentation complÃ¨te de tous vos exercices avec :
- Les commandes exÃ©cutÃ©es
- Les rÃ©sultats obtenus
- Les difficultÃ©s rencontrÃ©es
- Les observations

**Structure suggÃ©rÃ©e :**
```markdown
# Room 2 - Mes exercices HBase avancÃ©

## Exercice 1 : Versions

### Commandes exÃ©cutÃ©es :
[Vos commandes]

### RÃ©sultats :
[Ce que vous avez observÃ©]

### Observations :
- Les versions permettent de garder l'historique
- Par dÃ©faut, seulement la version la plus rÃ©cente est retournÃ©e
- On peut rÃ©cupÃ©rer une valeur Ã  un timestamp prÃ©cis

## Exercice 2 : Chargement de donnÃ©es IoT
[...]
```

### 2. `room-2_commandes.hbase`

Toutes vos commandes HBase dans un fichier rÃ©utilisable.

**Format :**
```hbase
# Room 2 - Commandes HBase avancÃ©

# Exercice 1 : Versions
create 'sensor_data', {NAME => 'readings', VERSIONS => 5}
put 'sensor_data', 'SENSOR001_20240101_100000', 'readings:temperature', '22.5'
# [...]
```

### 3. `room-2_observations.md`

Vos rÃ©flexions approfondies sur :
- Le choix des row keys pour les donnÃ©es temporelles
- L'utilisation des versions (quand et pourquoi)
- Les performances des diffÃ©rents filtres
- Les optimisations possibles

---

## âœ… Validation

Vous avez terminÃ© cette room quand :

- [ ] Vous avez crÃ©Ã© une table avec plusieurs versions et testÃ© la rÃ©cupÃ©ration des versions
- [ ] Vous avez chargÃ© au moins 10 lignes de donnÃ©es IoT dans HBase
- [ ] Vous avez utilisÃ© au moins 3 types de filtres diffÃ©rents (RowFilter, ValueFilter, SingleColumnValueFilter)
- [ ] Vous avez effectuÃ© des requÃªtes temporelles avec STARTROW/STOPROW
- [ ] Vous avez analysÃ© la distribution de vos donnÃ©es
- [ ] Vous avez identifiÃ© d'Ã©ventuels hotspots
- [ ] Vous avez proposÃ© des optimisations
- [ ] Vous avez crÃ©Ã© `room-2_exercices.md` avec toutes vos notes
- [ ] Vous avez crÃ©Ã© `room-2_commandes.hbase` avec toutes vos commandes
- [ ] Vous avez crÃ©Ã© `room-2_observations.md` avec vos rÃ©flexions

**Si toutes les cases sont cochÃ©es â†’ FÃ©licitations !** ðŸŽ‰

---

## ðŸš€ Prochaine Ã©tape

Une fois cette room terminÃ©e, vous pouvez passer Ã  **Room 3 : Introduction Ã  Hive**.

Dans la Room 3, vous apprendrez :
- Les bases de Hive et HiveQL
- Comment crÃ©er des tables Hive
- Comment charger des donnÃ©es depuis HDFS
- Les requÃªtes SQL de base

**Bon courage !** ðŸ’ª

---

## ðŸ“– Aide mÃ©moire rapide

### Commandes de versions
- `get 'table', 'row', {VERSIONS => 5}` = voir plusieurs versions
- `get 'table', 'row', {TIMESTAMP => ts}` = rÃ©cupÃ©rer Ã  un timestamp

### Filtres avancÃ©s
- `RowFilter` = filtrer sur les row keys
- `ValueFilter` = filtrer sur les valeurs (lent !)
- `SingleColumnValueFilter` = filtrer sur une colonne (rapide !)
- `ColumnPrefixFilter` = filtrer par prÃ©fixe de colonne
- `FilterList` = combiner plusieurs filtres

### RequÃªtes temporelles
- `STARTROW => '...'` = dÃ©but de la plage
- `STOPROW => '...'` = fin de la plage (exclue)

**Gardez ce fichier ouvert pendant vos exercices !** ðŸ“š
